{
	"$schema": "../../../src/schemas/section.json",
	"name": "method",
	"content": [
		[
			"heading",
			{
				"text": "案例做法",
				"level": 1
			}
		],
		[
			"paragraph",
			[
				"第一步：数据标注、预处理"
			]
		],
		[
			"paragraph",
			[
				"（1）预训练数据：预训练数据可以帮助大模型具备较为稳固的知识储备。部分数据来自网络上共享的开源预训练数据集，主要是官方发布的已经进行过初步数据清洗的公共数据。另一部分来自抓取的党媒网站的新闻、文件、公开数据库等。"
			]
		],
		[
			"paragraph",
			[
				"（2）指令数据：指令数据是用于大模型的微调训练数据集。可以使得大模型适应下游任务。指令数据一部分来自官方机构公布的指令数据，如复旦大学MOSS团队开源的中英文多轮对话数据，包含100万+数据，CLUE中文数据集等。"
			]
		],
		[
			"paragraph",
			[
				"（3）本地知识库：本地知识库可以帮助党媒大模型更好的扩充知识库。使用中文embedding模型，将本地党媒数据进行切分、分段、编码，以便进行相似查询。"
			]
		],
		[
			"paragraph",
			[
				"第二步：大模型预训练"
			]
		],
		[
			"paragraph",
			[
				"（1）首选需要选定预训练大模型基座，党媒大模型选用多种国产大模型，如Baichuan2, ChatGLM2,Qwen2等大模型。并配置预训练所需环境。"
			]
		],
		[
			"paragraph",
			[
				"（2）进行词表扩充。为了降低模型的训练难度，在原有的词表上进行扩充，将一些常见的中文token手动添加到原来的Tokenizer中，从而降低训练难度。"
			]
		],
		[
			"paragraph",
			[
				"（3）预训练。PreTraining 的思路很简单，就是输入文本，让模型做Next Token Prediction的任务。预训练过程中用到的方法有：数据源采样、数据预处理、模型结构。"
			]
		],
		[
			"paragraph",
			[
				"第三步：指令微调"
			]
		],
		[
			"paragraph",
			[
				"（1）准备好将上一步训练好的预训练模型，并配置好微调环境。"
			]
		],
		[
			"paragraph",
			[
				"（2）数据预处理。将第一步准备好的alpaca数据集转化为按行存储的Intruct格式数据，然后把数据划分为train.txt和valid.txt，保存在./data/example/路径下。"
			]
		],
		[
			"paragraph",
			[
				"（3）使用lora脚本进行预训练。第四步：模型评测及优化根据评测结果，对预训练数据集、微调数据集进行专项升级，然后重复步骤二、三、四。第五步：模型部署和调用。"
			]
		]
	]
}
